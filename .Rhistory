med_pop = median(total_pop))
# ----- (5) top_n() -----
# top_n() 함수와 Group_by()를 같이 활용하면 최중요지표를 보다 쉽게 추출할 수 있습니다.
# Group by region and find the greatest number of citizens who walk to work
counties_selected %>%
group_by(region) %>%
top_n(1, population)
# 실습 예제
counties_selected <- counties %>%
select(region, state, county, income)
# 문제, region & state 그룹에서 평균 Income이 가장 높은 것 1행씩을 추출하세요.
counties_selected %>%
group_by(region, state) %>%
# 평균 Income 요약하기
summarise(average_income = mean(income)) %>%
# 각 Region에서 평균 Income이 가장 Top에 있는 것 추출하기
top_n(1, average_income)
#### Step 5. 그 외 사용하는 주요 함수 소개 ####
# 이 부분에 대한 연습은 과제로 남겨둡니다.
# select & rename https://dplyr.tidyverse.org/reference/select.html
# transmute() https://dplyr.tidyverse.org/reference/mutate.html
# ungroup() https://dplyr.tidyverse.org/reference/group_by.html
# 위 함수를 사용하면 보다 더 효율적으로 피벗테이블을 만들 수 있습니다.
# End of Document
install.packages("dplyr")
#### Step 3. 데이터 처리와 관련된 주요 함수 소개 ####
# 데이터 가져오기
counties <- readxl::read_xlsx("/Volume/T7/git/kma/1_day_eda/data/counties.xslx", sheet = 1)
# Load packages
library(tidyverse)
library(viridis)
library(ggdark)
library(ggplot2)
circleFun <- function(center = c(0, 0), diameter = 1, npoints = 100){
r = diameter / 2
tt <- seq(0,2*pi,length.out = npoints)
xx <- center[1] + r * cos(tt)
yy <- center[2] + r * sin(tt)
return(data.frame(x = xx, y = yy))
}
dat <-
circleFun(c(1, -1), 2.3, npoints = 100)
ggplot(dat,aes(x, y)) +
geom_path()
genFun <- function(center = c(0, 0), npoints = 500, c1 = 2.5, c2 = -5, c3 = 4.28, c4 = 2.3){
t <- seq(0, 2*pi, length.out = npoints)
xx <- center[1] + c1*(sin(c2*t)*sin(c2*t))*(2^cos(cos(c3*c4*t)))
yy <- center[2] + c1*sin(sin(c2*t))*(cos(c3*c4*t)*cos(c3*c4*t))
a <- data.frame(x = xx, y = yy)
return(a)
}
dat <-
genFun(c(1,-1), npoints = 100)
ggplot(dat, aes(x, y)) +
geom_path()
dat <-
genFun(c(1,-1), npoints = 500, c1 = 5, c2 = -3, c3 = 5, c4 = 2)
ggplot(dat, aes(x, y)) +
geom_path()
dat <-
genFun(c(1,-1), npoints = 5000)
ggplot(dat, aes(x, y)) +
geom_line()
set.seed(1234)
dat <-
genFun(c(1,-1), npoints = 500)
dat %>%
\ggplot2(aes(x, y)) +
geom_point()
install.packages("ggplot2")
library(ggplot2)
glipmse(iris)
install.packages("glipmse")
library(glipmse)
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
glimpse(iris)
install.packages("glimpse)
iris <- iris
glimpse(iris)
install.packages("ggplot2")
library(ggplot2)
install.packages("glimpse")
library(glimpse)
iris <- iris
glimpse(iris)
iris <- iris
glimpse(iris)
cls
library(data.table)
library(curl)
View(iris)
View(iris)
# (4) 가장 기본적인 에러 확인
r_basics <- 2 ^ 3
# 에러 유형 확인 (실행 후 에러를 확인하세요!!)
r_basic
# Error: object 'r_basic' not found
R_basics
## 에러 연습
my_variable <- "나의 첫번째 변수"
my_varlable
# 4. 주요 변수 타입의 특징 공부
## 변수의 종류별 예시 코드
# 숫자형 변수
my_numeric <- 42
이름 <- c("홍길동", "심청이", "김길동", "성춘향")
나이 <- c(30, 20, 34, 50)
지각 <- c(TRUE, FALSE, FALSE, FALSE)
data.frame(이름 = 이름,
나이 = 나이,
지각 = 지각)
학생 <- data.frame(이름 = 이름,
나이 = 나이,
지각 = 지각)
print(학생)
str(학생)
학생 <- data.frame(이름 = 이름,
나이 = 나이,
지각 = 지각,
stringsAsFactors = FALSE)
str(학생)
#### 2. 파일 내보내기 ####
# 1. CSV 파일로 내보내기
getwd()
setwd('/Volume/T7/git/kma/')
write.csv(x = 학생, file = "data/학생.csv")
# 2. 엑셀파일로 내보내기
# install.packages("writexl")
writexl::write_xlsx(x = 학생, path = "data/학생.xlsx")
setwd("/Volumes/T7/git/kma")
install.packages("rvest")
install.packages("xml2")
library(rvest)
library(xml2)
html_df = read_html("data/intro.html", encoding = "utf-8")
html_df
xml_structure)html_df)
xml_structure(html_df)
library(rvest) # 중요
library(xml2)
## html 파일 불러오기
html_df = read_html("data/intro.html", encoding = "utf-8")
html_df
## xml_structure로 확인하기
xml_structure(html_df)
## 내부 구조 확인
html_df = read_html("data/intro2.html", encoding = "utf-8")
html_df
xml_structure(html_df)
## html_node 함수 사용
# 구 버전
html_df %>%
html_node('body')
# 신 버전
html_df %>%
html_element('body')
## html_elements or nodes 함수 사용
html_df %>%
html_elements('div p')
## Attribute 추출하기
html_df %>%
html_node('a') %>%
html_attr(name = 'href')
html_df %>%
html_nodes('a') %>%
html_attrs()
## table 태그 확인하기
html_df = read_html("data/intro3.html", encoding = "utf-8")
html_df %>%
html_table()
getwd()
res = GET(https://en.wikipedia.org/wiki/Anscombe%27s_quartet)
res = GET('https://en.wikipedia.org/wiki/Anscombe%27s_quartet')
# install.packages("xml2")
library(ggplot2)
library(dplyr)
library(dplyr)
library(ggplot2)
library(rvest)
library(xml2)
library(dplyr)
res = GET('https://en.wikipedia.org/wiki/Anscombe%27s_quartet')
print(x = res)
status_code(res)
html_df <- read_html("data/intro.html", encoding = "utf-8")
html_df
## xml_structure로 확인하기
xml_structure(html_df)
html_df <- read_html("data/intro2.html", encoding = "utf-8")
html_df %>%
html_children() %>%
html_text()
html_df %>%
html_node('body')
html_df %>%
html_nodes('div p')
html_df %>%
html_node('a') %>%
html_attr('href')
html_df %>%
html_nodes('a') %>%
html_attrs()
html_df <- read_html("data/intro3.html", encoding = "utf-8")
html_df %>%
html_table()
library(httr)
res = GET('https://en.wikipedia.org/wiki/Anscombe%27s_quartet')
print(x = res)
status_code(res)
library(lubridate)
library(httr)
library(urltools)
library(rvest)
library(tidyverse)
library(jsonlite)
# 테스트를 위해 오늘 날짜를 지정합니다.
today <- Sys.Date() %>% as.character()
# 퍼센트 인코딩 함수가 담긴 R 파일을 불러옵니다.
source(file = './code/encodings.R')
getBlogCnt <- function(searchWord, bgnDate, endDate) {
# HTTP 요청합니다.
res <- GET(url = 'https://section.blog.naver.com/ajax/SearchList.nhn',
query = list(countPerPage = '7',
currentPage = '1',
startDate = bgnDate,
endDate = endDate,
keyword = searchWord %>% pcntEncoding2Utf8() ),
add_headers(referer = 'https://section.blog.naver.com/BlogHome.nhn') )
# 응답 결과를 확인합니다.
print(x = res)
library(httr)
library(urltools)
library(stringr)
library(rvest)
library(httr)
library(urltools)
library(stringr)
library(rvest)
url <- "https://search.kyobobook.co.kr/web/search"
keyword = "데이터과학"
res <- GET(url = url,
query = list(vPstrKeyWord = keyword,
vPstrTab = "PRODUCT",
searchPcondition = 1,
currentPage = 1))
res %>%
read_html() %>%
html_node(css = "a#searchCategory_0") %>%
html_text() %>%
str_sub(start = 5, end = -1) %>% as.integer() -> total_pages
df = data.frame()
for (i in 1:total_pages) {
res <- GET(url = url,
query = list(vPstrKeyWord = keyword,
vPstrTab = "PRODUCT",
searchPcondition = 1,
currentPage = i))
kyobo_df = res %>%
read_html() %>%
html_node(css = "table.type_list") %>%
html_table()
df = rbind(df, kyobo_df)
}
View(df)
View(df)
View(kyobo_df)
View(df)
# list files
list.files("pdf/")
# 필요한 패키지를 불러옵니다.
library(httr)
library(urltools)
library(stringr)
library(rvest)
# pdf download
url = "http://consensus.hankyung.com/apps.analysis/analysis.list"
res <- GET(url = url,
query = list(sdate="2021-10-28",
edate="2021-10-28",
order_type="",
now_page=1))
# 테이블
res %>%
read_html(encoding = "EUC-KR") %>%
html_node(css = "div.table_style01") %>%
html_table() -> data01
# 첨부파일 링크
res %>%
read_html(encoding = "EUC-KR") %>%
html_nodes(css = "div.dv_input > a") %>%
html_attr("href") -> pdf_links
# pdf 타이틀
res %>%
read_html(encoding = "EUC-KR") %>%
html_nodes(css = "div.dv_input > a") %>%
html_attr("title") -> pdf_titles
# 데이터 추가
data01$첨부파일 = paste0("http://consensus.hankyung.com/", pdf_links)
data01$파일제목 = pdf_titles
# 다운로드
for (i in 1:nrow(data01)) {
download.file(url = data01$첨부파일[i],
destfile = paste0("pdf/", data01$파일제목[i]),
mode = "wb")
}
# list files
list.files("pdf/")
library(tabulizer)
library(pdftools)
library(magrittr)
library(stringr)
library(dplyr)
library(tidytext)
library(KoNLP)
library(wordcloud)
library(ggplot2)
file_lists = list.files("pdf/")
file_names = gsub(".pdf$", "", file_lists)
# text 추출
raw_txt = extract_text(file = paste0("pdf/", file_lists[1]), pages = 1)
article_df = data.frame(
file_names = file_names[1],
raw_text = raw_txt
)
# 데이터 프레임
articles = article_df %>%
mutate(clean_text = str_replace_all(raw_text, "[^가-힣]", " "),
clean_text = str_squish(clean_text))
# 단어 분리
noun <- extractNoun(articles$clean_text)
noun <- Filter(function(x) {nchar(x)>=2}, noun)
wordcount <- table(noun)
sort(wordcount, decreasing = T)
palete <- brewer.pal(9,"Set3")
wordcloud(names(wordcount),
freq=wordcount,
scale=c(5,1),
rot.per=0.25,
min.freq=1,
random.order=F,
random.color=T,
family = "AppleGothic",
colors=palete)
# 테이블 추출
df = extract_areas(file = paste0("pdf/", file_lists[1]), pages = 1)
df %>% as.data.frame() -> data
names(data) <- as.character(unlist(data[1, ]))
data = data[-1, ]
library(tabulizer)
library(pdftools)
library(magrittr)
library(stringr)
library(dplyr)
library(tidytext)
library(KoNLP)
library(wordcloud)
library(ggplot2)
install.packages("KoNLP")
install.packages("koNLP")
library(lubridate)
library(httr)
library(urltools)
library(rvest)
library(tidyverse)
library(jsonlite)
# 테스트를 위해 오늘 날짜를 지정합니다.
today <- Sys.Date() %>% as.character()
# 퍼센트 인코딩 함수가 담긴 R 파일을 불러옵니다.
source(file = './code/encodings.R')
# 검색어와 조회시작일자, 조회종료일자를 입력하면 조건에 맞는 블로그의 수를 반환하는
# 사용자 정의 함수를 만듭니다.
getBlogCnt <- function(searchWord, bgnDate, endDate) {
# HTTP 요청합니다.
res <- GET(url = 'https://section.blog.naver.com/ajax/SearchList.nhn',
query = list(countPerPage = '7',
currentPage = '1',
startDate = bgnDate,
endDate = endDate,
keyword = searchWord %>% pcntEncoding2Utf8() ),
add_headers(referer = 'https://section.blog.naver.com/BlogHome.nhn') )
# 응답 결과를 확인합니다.
# print(x = res)
# 응답 바디에 있는 ")]}',"을 제거하고 fromJSON() 함수에 할당합니다.
json <- res %>%
as.character() %>%
str_remove(pattern = "\\)\\]\\}\\',") %>%
fromJSON()
# 블로그 수를 추출한 다음 숫자 벡터로 변환합니다.
totalCount <- json$result$totalCount %>% as.numeric()
# 결과를 반환합니다.
return(totalCount)
}
# 오늘 날짜로 테스트합니다.
getBlogCnt(searchWord = '암호화폐',
bgnDate = today,
endDate = today)
# 검색어와 조회시작일자, 조회종료일자, 페이지수를 입력하면 조건에 맞는 블로그
# 데이터를 수집하여 데이터프레임으로 반환하는 사용자 정의 함수를 만듭니다.
getBlogDf <- function(searchWord, bgnDate, endDate, page = 1) {
# HTTP 요청합니다.
res <- GET(url = 'https://section.blog.naver.com/ajax/SearchList.nhn',
query = list(countPerPage = '7',
currentPage = page,
startDate = bgnDate,
endDate = endDate,
keyword = searchWord %>% pcntEncoding2Utf8() ),
add_headers(referer = 'https://section.blog.naver.com/BlogHome.nhn') )
# 응답 결과를 확인합니다.
# print(x = res)
# 응답 바디에 있는 ")]}',"을 제거하고 fromJSON() 함수에 할당합니다.
json <- res %>%
as.character() %>%
str_remove(pattern = "\\)\\]\\}\\',") %>%
fromJSON()
# 데이터프레임을 출력합니다.
df <- json$result$searchList
# 필요한 컬럼만 남깁니다.
df <- df[, c('blogId', 'postUrl', 'noTagTitle', 'contents', 'nickName', 'blogName', 'addDate')]
# 간단한 전처리를 실시합니다.
# contents 컬럼에 있는 태그와 불필요한 기호(&quot;)를 제거합니다.
df$contents <- df$contents %>% str_remove_all(pattern = '<.+?>|&quot;')
# addDate 컬럼에 날짜 대신 숫자 벡터로 들어 있으므로 1000으로 나눈 후,
# POSIXct로 속성을 바꿔줍니다.
df$addDate <- as.POSIXct(x = df$addDate / 1000, origin = '1970-01-01')
# 결과를 반환합니다.
return(df)
}
# 오늘 날짜로 테스트합니다.
df <- getBlogDf(searchWord = '암호화폐',
bgnDate = today,
endDate = today,
page = 1)
# 결과를 출력합니다.
print(x = df)
# 검색어와 조회시작일자, 조회종료일자를 지정하면 해당 조건의 모든 블로그 데이터를
# 수집하는 반복문이 포함된 사용자 정의 함수를 만듭니다.
getAllBlogDf <- function(searchWord, bgnDate, endDate) {
# 조건에 맞는 블로그 수를 가져옵니다.
blogCnt <- getBlogCnt(searchWord = searchWord, bgnDate = bgnDate, endDate = endDate)
# 페이지 수를 계산합니다.
pages <- ceiling(x = blogCnt / 7)
# 블로그 수와 페이지 수를 출력합니다.
cat('> 블로그 수는', blogCnt, '& 페이지 수는', pages, '입니다.\n')
# 만약 블로그의 수가 0이면 아래 라인을 실행하지 않습니다.
if (blogCnt >= 1) {
# 최종 결과 객체를 빈 데이터프레임으로 생성합니다.
result <- data.frame()
# 반복문을 실행합니다.
for (page in 1:pages) {
# 현재 진행상황을 출력합니다.
cat('>> 현재', page, '페이지 실행 중입니다.\n')
# 해당 페이지의 블로그 데이터를 수집한 다음 df에 할당합니다.
df <- getBlogDf(searchWord = searchWord,
bgnDate = bgnDate,
endDate = endDate,
page = page)
# 최종 결과 객체에 추가합니다.
result <- rbind(result, df)
# 1초간 멈춥니다.
Sys.sleep(time = 1)
}
# 최종 결과를 반환합니다.
return(result)
}
}
# 오늘 날짜로 테스트합니다.
blogData <- getAllBlogDf(searchWord = '암호화폐',
bgnDate = today,
endDate = today)
# 결과를 출력합니다.
print(x = blogData)
# 오늘 날짜로 테스트합니다.
blogData <- getAllBlogDf(searchWord = '암호화폐',
bgnDate = today,
endDate = today)
# 검색어를 설정합니다.
keyword <- '암호화폐'
# dates 객체를 출력합니다.
print(x = dates)
# 반복문을 실행합니다.
for (date in dates) {
# 반복문 안에서 date가 날짜 벡터에서 숫자 벡터로 자동 변환되는
# 문제가 발생하므로 다시 날짜 벡터로 강제 변환합니다.
date <- date %>% as.Date(origin = '1970-01-01')
# 현재 진행상황을 출력합니다.
date4print <- format(x = date, format = '%Y년 %m월 %d일에')
cat('현재', date4print, '등록된 블로그를 수집하고 있습니다.\n')
# 해당 일자에 등록된 모든 블로그를 수집합니다.
blogData <- getAllBlogDf(searchWord = keyword,
bgnDate = date,
endDate = date)
# 최종 결과 객체에 추가합니다.
result <- rbind(result, blogData)
# 개행을 추가합니다.
cat('\n')
# 1초간 멈춥니다.
Sys.sleep(time = 1)
}
# 결과를 출력합니다.
print(x = blogData)
# 조회시작일과 조회종료일로부터 벡터를 생성합니다.
dates <- seq(from = ymd('2021-01-01'), to = ymd('2021-10-03'), by = '1 day')
# 최종 결과 객체를 빈 데이터프레임으로 생성합니다.
result <- data.frame()
# 최종 결과 객체의 구조를 파악합니다.
str(object = result)
View(df)
library(dplyr)
library(tuber)
library(ggplot2)
