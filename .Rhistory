#### 단계 4. 모형 해석 ####
summary(chatModel)
# Likelihood ratio test : chisq = 278.52 (p.value = < 2.22e-16)
# 로그 가능도는 교재에서 본 것처럼, 자료에서 설명되지 않은 변동이 어느 정도인지 말해주는 측도이다.
# 로그 가능도의 차이 또는 변화는 새 변수가 모형을 어느 정도나 설명하는지를 나타낸다.
# 기저 모형만 분석해보자..
chatBase <- mlogit(Success ~ 0 | Sex, data = mlChat, reflevel = "No response/Walk Off")
summary(chatBase)
data.frame(exp(chatModel$coefficients))
# 이 값은 승산비이다.
# 승산비에 대한 설명은 교재를 참고한다.
# 어떻게 해석해야 할까
# 우선 기저변수인 No Response/walk off와 범주를 비교한 결과를 말한다.
# 이 계수들의 신뢰구간은 confint() 함수로 구할 수 있다.
exp(confint(chatModel))
# 각각의 해석 또한 교재를 참고한다.
# 각각의 해석 또한 교재를 참고한다.
# 각각의 해석 또한 교재를 참고한다.
# 각각의 해석 또한 교재를 참고한다.
#### I. caret 패키지 활용 머신러닝 모형 개발 ####
#### 단계 1. 병렬처리를 위한 패키지 불러오기  ####
library(caret) # 머신러닝을 위한 패키지
library(tidyverse) # 데이터 핸들링 및 시각화를 위한 패키지
library(doParallel) # 병렬처리를 위한 패키지
detectCores() # 현재 자기 컴퓨터의 코어 개수를 반환한다
cl <- parallel::makeCluster(2, setup_timeout = 0.5)
registerDoParallel(cl)
#### 단계 2. 데이터 가져오기  ####
setwd("~/Documents/R_edu")
setwd("/Volumes/T7/git/kma")
detectCores() # 현재 자기 컴퓨터의 코어 개수를 반환한다
cl <- parallel::makeCluster(2, setup_timeout = 0.5)
registerDoParallel(cl)
#### 단계 2. 데이터 가져오기  ####
loan_data <- read.csv("data/cleaned_loan_data.csv", stringsAsFactors = FALSE) # 29091 8
#### 단계 3. 데이터 전처리 ####
# 결측치 확인
sapply(loan_data, function(x) sum(is.na(x)))
# 중복값 확인
loan_data %>% duplicated() %>% sum() # 374개 확인
loan_data2 <- loan_data %>% distinct()
# 데이터 타입 확인
glimpse(loan_data)
# 사실 여기부터 모형 개발에 목적에 맞게 기저 범주를 하나씩 설정하며 봐야 하지만. 이 부분은 수강생 분들에게 맡기겠다.
loan_data2$loan_status <- factor(loan_data2$loan_status, levels = c(0, 1), labels = c("non_default", "default"))
loan_data2$grade <- as.factor(loan_data2$grade)
loan_data2$home_ownership <- as.factor(loan_data2$home_ownership)
# 한꺼번에 하고 싶습니다.
loan_data2 <- loan_data2 %>%
mutate_if(is.character, as.factor)
glimpse(loan_data2)
#### 단계 4. 데이터 분리 ####
set.seed(2018)
inx   <- createDataPartition(loan_data2$loan_status, p = 0.6, list = F)
train <- loan_data2[ inx, ]
test  <- loan_data2[-inx, ]
#### 단계 5. 모형 controler 개발 ####
# caret 패키지의 특징
control <- trainControl(
method  = "repeatedcv",
number  = 10, # 10겹
repeats = 3, # 3번
search  = "grid",
classProbs = TRUE)
#### 단계 6. 데이터의 통계적인 전처리 ####
# feature engineering
# 각각에 대한 설명은 4일차에 진행함
preProc <- c("BoxCox",
"center",
"scale",
"spatialSign",
"corr",
"zv")
## define x, y
## logistic regression
set.seed(2018)
glimpse(train)
frml <- loan_status ~ loan_amnt + grade + home_ownership + annual_inc + age + emp_cat + ir_cat
#### 단계 7.1. 모형 개발 - 로지스틱회귀 분석 ####
logis <- train(
frml,
data = train,
method     = "glm",
metric     = "Accuracy",
trControl  = control,
preProcess = preProc)
logis
#### 데이터 전처리 시 기본적원 방법 정리 ####
#### (2) 데이터 불러오기 ####
# setwd("~/Documents/R_edu")
loan_data <- read.csv("data/raw_loan_data.csv", stringsAsFactors = FALSE)
str(loan_data)
#### (3) 데이터 탐색하기 ####
# Load the gmodels package
library(gmodels)
# 종속변수가 되는 loan_status에 대해 데이터를 탐색합니다.
CrossTable(loan_data$loan_status)
CrossTable(x = loan_data$home_ownership,
y = loan_data$loan_status,
prop.r = TRUE, prop.c = FALSE, prop.t = FALSE, prop.chisq = FALSE)
?CrossTable
#### (4) 데이터 시각화 하기 ####
library(ggplot2)
summary(loan_data)
# 수치형 데이터 - 히스토그램으로 대출금액 파악하기
ggplot(loan_data, aes(x = loan_amnt)) +
geom_histogram(bins = 200) +
labs(title = "Total Amount of Loan") +
theme_minimal()
ggplot(loan_data, aes(x = int_rate)) +
geom_histogram(bins = 200) +
labs(title = "Interest Rate of Loan") +
theme_minimal()
ggplot(loan_data, aes(x = annual_inc / 100)) +
geom_histogram(bins = 200) +
scale_x_log10() +
labs(title = "Annual Income") +
theme_minimal()
ggplot(loan_data, aes(x = age)) +
geom_histogram() +
labs(title = "Age") +
theme_minimal()
#### (5) 이상치 제거 ####
# 시각화의 목적은 크게 두가지가 있다.
# 변수간의 관계성, 분포도 등 확인이 필요하다.
# 가장 좋은 것 중 하나는, missing data를 잡아내거나, 이상치를 잡아낼 수 있다.
# 산점도를 확인해보자
ggplot(loan_data, aes(x = age, y = annual_inc / 100)) +
geom_point(alpha = .3) +
theme_minimal()
# (1) 방법으로 제거
# 상식적으로 140세는 존재할 수 없다. (현재 의학으로는..)
# 다행히, 데이터가 1개만 존재하기 때문에, 삭제해도 무방하다.
# 이상치인 데이터의 Index를 찾아본다.
index_highage <- which(loan_data$age > 122) # 이 코드는 특정 Index를 유용하기 때문에 꼭 참고한다.
index_highage # 19486번째 사람인 것 확인
# 새로운 데이터를 만든다. loan_data2 로 저장한다.
loan_data2 <- loan_data[-index_highage, ]
# (2) 방법으로 제거
# 사분위수 Q3 + 1.5 * IQR 보다 큰것을 이상치로 판단 후 제거
outlier_cutoff <- quantile(loan_data$annual_inc, 0.75) + 1.5 * IQR(loan_data$annual_inc)
print(outlier_cutoff)
index_outlier_ROT <- which(loan_data$annual_inc > outlier_cutoff)
length(index_outlier_ROT) # 이상치로 측정된 전체 Index는 1382개수로 확인됨
loan_data_ROT <- loan_data[-index_outlier_ROT, ] # 이상치 제거한 데이터
rm(loan_data_ROT) # 이 데이터는 사용하지 않을 것이기 때문에 제거
#### (6) 결측치 처리 방법 ####
summary(loan_data2)
#### (6-1) 결측치 제거 ####
## 행 제거
# 결측치에 해당하는 Index 확인
na_index <- which(is.na(loan_data$int_rate))
length(na_index)
# Index 활용 제거
loan_data_delrow_na <- loan_data[-na_index, ]
## 변수 제거
loan_data_delcol_na <- loan_data
loan_data_delcol_na$int_rate <- NULL # 간단하게 변수 제거
#### (6-2) 중간값 대치 ####
# 우선, 결측치를 제외한 중간값을 구한다.
median_ir <- median(loan_data$int_rate, na.rm = TRUE)
# 다른 데이터로 변환
loan_data_replace <- loan_data
# 결측치 index에 중간값 대치
loan_data_replace$int_rate[na_index] <- median_ir
# 요약 통계량으로 결측치 유무 재확인
summary(loan_data_replace$int_rate)
#### (6-3) 결측치를 사용하기 ####
# 단, 중요한 것은 수치형이나 범주형이나, NA를 하나의 값으로 생각하고 치환하는 것이 핵심 포인트입니다.
loan_data$emp_cat <- rep(NA, length(loan_data$emp_length))
loan_data$emp_cat[which(loan_data$emp_length <= 15)] <- "0-15"
loan_data$emp_cat[which(loan_data$emp_length > 15 & loan_data$emp_length <= 30)] <- "15-30"
loan_data$emp_cat[which(loan_data$emp_length > 30 & loan_data$emp_length <= 45)] <- "30-45"
loan_data$emp_cat[which(loan_data$emp_length > 45)] <- "45+"
loan_data$emp_cat[which(is.na(loan_data$emp_length))] <- "Missing"
loan_data$emp_cat <- as.factor(loan_data$emp_cat)
# 근속년수에 따른 데이터를 변환하면 'Missing'으로 하나의 의미있는 값으로 치환할 수 있습니다.
# End of Document
# End of Document
View(loan_data)
#### (6) 결측치 처리 방법 ####
summary(loan_data2)
#### (6-1) 결측치 제거 ####
## 행 제거
# 결측치에 해당하는 Index 확인
na_index <- which(is.na(loan_data$int_rate))
length(na_index)
# Index 활용 제거
loan_data_delrow_na <- loan_data[-na_index, ]
## 변수 제거
loan_data_delcol_na <- loan_data
loan_data_delcol_na$int_rate <- NULL # 간단하게 변수 제거
#### (6-2) 중간값 대치 ####
# 우선, 결측치를 제외한 중간값을 구한다.
median_ir <- median(loan_data$int_rate, na.rm = TRUE)
# 다른 데이터로 변환
loan_data_replace <- loan_data
# 결측치 index에 중간값 대치
loan_data_replace$int_rate[na_index] <- median_ir
# 요약 통계량으로 결측치 유무 재확인
summary(loan_data_replace$int_rate)
#### (6-3) 결측치를 사용하기 ####
# 단, 중요한 것은 수치형이나 범주형이나, NA를 하나의 값으로 생각하고 치환하는 것이 핵심 포인트입니다.
loan_data$emp_cat <- rep(NA, length(loan_data$emp_length))
loan_data$emp_cat[which(loan_data$emp_length <= 15)] <- "0-15"
loan_data$emp_cat[which(loan_data$emp_length > 15 & loan_data$emp_length <= 30)] <- "15-30"
loan_data$emp_cat[which(loan_data$emp_length > 30 & loan_data$emp_length <= 45)] <- "30-45"
loan_data$emp_cat[which(loan_data$emp_length > 45)] <- "45+"
loan_data$emp_cat[which(is.na(loan_data$emp_length))] <- "Missing"
loan_data$emp_cat <- as.factor(loan_data$emp_cat)
# 로지스틱 회귀분석의 특징은 다음과 같습니다.
# 분석목적: 종속변수와 독립변수 간의 관계를 통해서 예측 모델을 생성합니다.
# 회귀분석과의 차이점: 종속변수는 반드시 범주형 변수이어야 합니다.
# 이항형: Yes/No, 다항형: 예) Iris의 Species 칼럼
# 정규성: 정규분포 대신에 이항분포를 따릅니다.
# 로짓변환: 종속변수의 출력범위를 0과 1로 조정하는 과정을 의미합니다.
#### I. 이항분류 기존 방식 ####
#### 단계 1. 데이터 가져오기 ####
# setwd("~/Documents/R_edu") 사용하지 않습니다.
loan_data <- read.csv("data/cleaned_loan_data.csv", stringsAsFactors = FALSE)
head(loan_data)
str(loan_data)
# 데이터 변환의 이유, 1, 0은 숫자가 아니다. 범주형이다.
loan_data$loan_status <- as.factor(loan_data$loan_status)
#### 단계 2. 데이터 분리 ####
library(caret)
inTrain <- createDataPartition(y = loan_data$loan_status, p=0.6, list = FALSE)
train_loan <- loan_data[inTrain,]
test_loan  <- loan_data[-inTrain,]
dim(train_loan)
#### 단계 3. 모형 개발 ####
options(scipen = 100)
null_modle <- glm(loan_status ~ 1, family = "binomial", data = loan_data)
log_model <- glm(loan_status ~ loan_amnt + grade, family = "binomial", data = loan_data)
summary(log_model)
# 카이제곱 통계량 및 유의확률 구하기
anova(null_modle, log_model, test = "Chisq")
# 승산비 구하는 법
t(exp(log_model$coefficients))
# 신뢰구간 구하기
exp(confint(log_model))
#### 단계 4. 모형 예측 ####
pred <- predict(log_model, newdata = test_loan, type = "response")
# type = "response" 속성은 예측 결과를 0~1사이의 확률값으로 반환한다.
#### 단계 5. 예측치를 이항형으로 변환 ####
# 예측치가 0.15 이상이면 1, 0.15 미만이면 0
# 일단, 0.15인지는 메뉴얼 참조
result_pred <- ifelse(pred > 0.15, 1, 0)
#### 단계 6. 분류 정확도 계산 ####
table(test_loan$loan_status, result_pred)
#### 단계 7. 분류 정확도 계산 ####
# 혼동 매트릭스에 대한 설명은 교재 참조
# 정확도 계산
(5065 + 321) / (5065 + 321 + 1401 + 485)
#### 단계 8. ROC Curve를 이용한 모델 평가 & AUC ####
library(ROCR)
par(mfrow = c(1,1))
pr <- prediction(pred, test_loan$loan_status)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
abline(0, 1, col = "red")
## --
# AUC = Area Under Curve의 뜻으로
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]; auc
#### III. 다항 로지스틱 회귀분석 해석 및 보고서 작성 ####
#### 단계 1. 패키지 설치 ####
library(dfidx)
library(mlogit) # 다항 로지스틱 회귀
library(dplyr)
#### 단계 2. 데이터 불러오기 ####
# setwd("~/Documents/R_edu") 사용하지 않습니다.
chatData <- read.csv('data/chat-up_lines.csv', header = TRUE) %>%
mutate_if(is.character, as.factor)
head(chatData)
str(chatData)
#### 단계 3. 데이터 처리 ####
# Gender에서 male이 기저 범주가 되도록 한다.
# 연구의 목적은 대화문에 대한 여성의 반응이 남성의 반응과는 다르다는 점에 기초함.
# 이러한 변경을 통해서, 주 효과들에 대한 매개변수 추정값들에 영향을 미치기는 하나, 우리의 관심사인 상호작용 항들에는 영향이 미미하다.
chatData$Gender <- relevel(chatData$Gender, ref = 2)
# 데이터 변경
# 새 데이터 프레임 <- mlogit.data(기존데이터프레임, choice = "결과변수", shape = "wide" 또는 "long")
mlChat <- mlogit.data(chatData, choice = "Success", shape = "wide")
head(mlChat)
str(mlChat)
# 위 표에 대한 설명은 교재 참조
chatModel <- mlogit(Success ~ 1 | Good_Mate + Funny + Gender + Sex + Gender:Sex + Funny:Gender,
data = mlChat,
reflevel = "No response/Walk Off",
index = c("chid", "alt"))
#### 단계 4. 모형 해석 ####
summary(chatModel)
# Likelihood ratio test : chisq = 278.52 (p.value = < 2.22e-16)
# 로그 가능도는 교재에서 본 것처럼, 자료에서 설명되지 않은 변동이 어느 정도인지 말해주는 측도이다.
# 로그 가능도의 차이 또는 변화는 새 변수가 모형을 어느 정도나 설명하는지를 나타낸다.
# 기저 모형만 분석해보자..
chatBase <- mlogit(Success ~ 0 | Sex, data = mlChat, reflevel = "No response/Walk Off")
summary(chatBase)
data.frame(exp(chatModel$coefficients))
# 이 값은 승산비이다.
# 승산비에 대한 설명은 교재를 참고한다.
# 어떻게 해석해야 할까
# 우선 기저변수인 No Response/walk off와 범주를 비교한 결과를 말한다.
# 이 계수들의 신뢰구간은 confint() 함수로 구할 수 있다.
exp(confint(chatModel))
# 각각의 해석 또한 교재를 참고한다.
#### 단계 3. 데이터 전처리 ####
# 결측치 확인
sapply(loan_data, function(x) sum(is.na(x)))
# 중복값 확인
loan_data %>% duplicated() %>% sum() # 374개 확인
loan_data2 <- loan_data %>% distinct()
# 데이터 타입 확인
glimpse(loan_data)
# 사실 여기부터 모형 개발에 목적에 맞게 기저 범주를 하나씩 설정하며 봐야 하지만. 이 부분은 수강생 분들에게 맡기겠다.
loan_data2$loan_status <- factor(loan_data2$loan_status, levels = c(0, 1), labels = c("non_default", "default"))
loan_data2$grade <- as.factor(loan_data2$grade)
loan_data2$home_ownership <- as.factor(loan_data2$home_ownership)
# 한꺼번에 하고 싶습니다.
loan_data2 <- loan_data2 %>%
mutate_if(is.character, as.factor)
glimpse(loan_data2)
#### 단계 4. 데이터 분리 ####
set.seed(2018)
inx   <- createDataPartition(loan_data2$loan_status, p = 0.6, list = F)
train <- loan_data2[ inx, ]
test  <- loan_data2[-inx, ]
#### 단계 5. 모형 controler 개발 ####
# caret 패키지의 특징
control <- trainControl(
method  = "repeatedcv",
number  = 10, # 10겹
repeats = 3, # 3번
search  = "grid",
classProbs = TRUE)
#### 단계 6. 데이터의 통계적인 전처리 ####
# feature engineering
# 각각에 대한 설명은 4일차에 진행함
preProc <- c("BoxCox",
"center",
"scale",
"spatialSign",
"corr",
"zv")
## define x, y
## logistic regression
set.seed(2018)
glimpse(train)
frml <- loan_status ~ loan_amnt + grade + home_ownership + annual_inc + age + emp_cat + ir_cat
#### 단계 7.1. 모형 개발 - 로지스틱회귀 분석 ####
logis <- train(
frml,
data = train,
method     = "glm",
metric     = "Accuracy",
trControl  = control,
preProcess = preProc)
logis
# 이전버전
# saveRDS(logis, "R_NCS_2020/3_day/model/logis_model.rds") Version 3.6.3 까지 적용
save(logis, file = "model/logis_model.RData")
# 이전버전
# saveRDS(logis, "R_NCS_2020/3_day/model/logis_model.rds") Version 3.6.3 까지 적용
save(logis, file = "model/logis_model.RData")
#### 단계 7.2. 모형 개발 - 의사결정나무 ####
modelLookup("rpart")
# Model Tunning Parameter
# 4일차에 설명 예정
rpartGrid <- expand.grid(cp = c(0.001, 0.01, 0.1))
set.seed(2018)
rpt <- train(
frml,
data = train,
method     = "rpart",
metric     = "Accuracy",
trControl  = control,
preProcess = preProc,
tuneGrid   = rpartGrid)
rpt
ggplot(rpt)
save(rpt, file = "model/rpt_model.RData")
#### 단계 7.3. 모형 개발 - 랜덤포레스트 ####
modelLookup("rf")
rftGrid <- expand.grid(mtry = c(3, 5, 8)) # sqrt(p)
set.seed(2020)
# system.time은 시간을 재기 위한 것
system.time(
rft <- train(
frml,
data = train,
method     = "rf",
metric     = "Accuracy",
trControl  = control,
preProcess = preProc,
tuneGrid   = rftGrid,
ntree      = 500)
)
rft
ggplot(rft)
# 랜덤포레스트의 경우 연산이 많아 시간이 오래걸리므로
# 모형을 저장하는 것이 중요함
# saveRDS(rft, "../model/rft_model.rds")
save(rft, file = "model/rft_model.RData")
#### 단계 7.4. 모형 개발 - GBM(Stochastic Gradient Boosting Model) ####
modelLookup("gbm")
gbmGrid <- expand.grid(n.trees = 500,
interaction.depth = 30,
shrinkage = c(0.01, 0.1),
n.minobsinnode = 30)
set.seed(2020)
# system.time은 시간을 재기 위한 것
system.time(
gbm <- train(
frml,
data = train,
method     = "gbm",
metric     = "Accuracy",
trControl  = control,
preProcess = preProc,
tuneGrid   = gbmGrid,
verbose    = F)
)
rft
ggplot(rft)
# 랜덤포레스트의 경우 연산이 많아 시간이 오래걸리므로
# 모형을 저장하는 것이 중요함
# saveRDS(rft, "../model/rft_model.rds")
save(rft, file = "model/rft_model.RData")
#### 단계 7.4. 모형 개발 - GBM(Stochastic Gradient Boosting Model) ####
modelLookup("gbm")
gbmGrid <- expand.grid(n.trees = 500,
interaction.depth = 30,
shrinkage = c(0.01, 0.1),
n.minobsinnode = 30)
set.seed(2020)
# system.time은 시간을 재기 위한 것
system.time(
gbm <- train(
frml,
data = train,
method     = "gbm",
metric     = "Accuracy",
trControl  = control,
preProcess = preProc,
tuneGrid   = gbmGrid,
verbose    = F)
)
gbm
ggplot(gbm)
# 부스팅의 경우 연산이 많아 시간이 오래걸리므로
# 모형을 저장하는 것이 중요함
# saveRDS(gbm, "../model/gbm_model.rds")
save(gbm, file = "model/gbm_model.RData")
#### 단계 7.5. 모형 개발 - avNNet(인공신경망) ####
modelLookup("avNNet")
nnetGrid <- expand.grid(size = c(1, 5, 9),
decay = c(0.01, 0.1),
bag = F)
maxSize <- max(nnetGrid$size)
numWts <- 10 * (maxSize * (ncol(train) + 1) + 10 + 1)
set.seed(2020)
system.time(
snn <- train(
frml,
data = train,
method     = "avNNet",
metric     = "Accuracy",
trControl  = control,
preProcess = preProc,
tuneGrid   = nnetGrid,
maxit      = 500,
linout     = F,
trace      = F,
MaxNWts    = numWts)
)
??doParallel
snn
ggplot(snn)
# 인공신경망의 경우 연산이 많아 시간이 오래걸리므로
# 모형을 저장하는 것이 중요함
# saveRDS(snn, "../model/snn_model.rds")
save(snn, file = "model/snn_model.RData")
# R 4.0.0 이후 버전
load("model/logis_model.RData")
load("model/rpt_model.RData")
load("model/rft_model.RData")
load("model/gbm_model.RData")
load("model/snn_model.RData")
## compare models
resamps <- resamples(
list(glm = logis,
rpt = rpt,
rft = rft,
gbm = gbm,
snn = snn))
summary(resamps)
bwplot(resamps, layout = c(2, 1))
# 단계 9. 모형 예측 및 AUC
pred_snn <- predict(snn, test, type = "prob")
pred_snn$loan_status <- ifelse(pred_snn$non_default > 0.85, 0, 1) # cut-off를 조정하며 맞춰보자
pred_snn$loan_status <- factor(pred_snn$loan_status, levels = c(0, 1), labels = c("non_default", "default"))
confusionMatrix(pred_snn$loan_status, test$loan_status, positive = "non_default")
library(ROCR)
pr <- prediction(as.numeric(pred_snn$loan_status) - 1, as.numeric(test$loan_status) - 1)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
abline(0, 1, col = "red")
## --
# AUC = Area Under Curve의 뜻으로
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]; auc
