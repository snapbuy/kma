writexl::write_xlsx(x = 학생, path = "data/학생.xlsx")
setwd("/Volumes/T7/git/kma")
install.packages("rvest")
install.packages("xml2")
library(rvest)
library(xml2)
html_df = read_html("data/intro.html", encoding = "utf-8")
html_df
xml_structure)html_df)
xml_structure(html_df)
library(rvest) # 중요
library(xml2)
## html 파일 불러오기
html_df = read_html("data/intro.html", encoding = "utf-8")
html_df
## xml_structure로 확인하기
xml_structure(html_df)
## 내부 구조 확인
html_df = read_html("data/intro2.html", encoding = "utf-8")
html_df
xml_structure(html_df)
## html_node 함수 사용
# 구 버전
html_df %>%
html_node('body')
# 신 버전
html_df %>%
html_element('body')
## html_elements or nodes 함수 사용
html_df %>%
html_elements('div p')
## Attribute 추출하기
html_df %>%
html_node('a') %>%
html_attr(name = 'href')
html_df %>%
html_nodes('a') %>%
html_attrs()
## table 태그 확인하기
html_df = read_html("data/intro3.html", encoding = "utf-8")
html_df %>%
html_table()
getwd()
res = GET(https://en.wikipedia.org/wiki/Anscombe%27s_quartet)
res = GET('https://en.wikipedia.org/wiki/Anscombe%27s_quartet')
# install.packages("xml2")
library(ggplot2)
library(dplyr)
library(dplyr)
library(ggplot2)
library(rvest)
library(xml2)
library(dplyr)
res = GET('https://en.wikipedia.org/wiki/Anscombe%27s_quartet')
print(x = res)
status_code(res)
html_df <- read_html("data/intro.html", encoding = "utf-8")
html_df
## xml_structure로 확인하기
xml_structure(html_df)
html_df <- read_html("data/intro2.html", encoding = "utf-8")
html_df %>%
html_children() %>%
html_text()
html_df %>%
html_node('body')
html_df %>%
html_nodes('div p')
html_df %>%
html_node('a') %>%
html_attr('href')
html_df %>%
html_nodes('a') %>%
html_attrs()
html_df <- read_html("data/intro3.html", encoding = "utf-8")
html_df %>%
html_table()
library(httr)
res = GET('https://en.wikipedia.org/wiki/Anscombe%27s_quartet')
print(x = res)
status_code(res)
library(lubridate)
library(httr)
library(urltools)
library(rvest)
library(tidyverse)
library(jsonlite)
# 테스트를 위해 오늘 날짜를 지정합니다.
today <- Sys.Date() %>% as.character()
# 퍼센트 인코딩 함수가 담긴 R 파일을 불러옵니다.
source(file = './code/encodings.R')
getBlogCnt <- function(searchWord, bgnDate, endDate) {
# HTTP 요청합니다.
res <- GET(url = 'https://section.blog.naver.com/ajax/SearchList.nhn',
query = list(countPerPage = '7',
currentPage = '1',
startDate = bgnDate,
endDate = endDate,
keyword = searchWord %>% pcntEncoding2Utf8() ),
add_headers(referer = 'https://section.blog.naver.com/BlogHome.nhn') )
# 응답 결과를 확인합니다.
print(x = res)
library(httr)
library(urltools)
library(stringr)
library(rvest)
library(httr)
library(urltools)
library(stringr)
library(rvest)
url <- "https://search.kyobobook.co.kr/web/search"
keyword = "데이터과학"
res <- GET(url = url,
query = list(vPstrKeyWord = keyword,
vPstrTab = "PRODUCT",
searchPcondition = 1,
currentPage = 1))
res %>%
read_html() %>%
html_node(css = "a#searchCategory_0") %>%
html_text() %>%
str_sub(start = 5, end = -1) %>% as.integer() -> total_pages
df = data.frame()
for (i in 1:total_pages) {
res <- GET(url = url,
query = list(vPstrKeyWord = keyword,
vPstrTab = "PRODUCT",
searchPcondition = 1,
currentPage = i))
kyobo_df = res %>%
read_html() %>%
html_node(css = "table.type_list") %>%
html_table()
df = rbind(df, kyobo_df)
}
View(df)
View(df)
View(kyobo_df)
View(df)
# list files
list.files("pdf/")
# 필요한 패키지를 불러옵니다.
library(httr)
library(urltools)
library(stringr)
library(rvest)
# pdf download
url = "http://consensus.hankyung.com/apps.analysis/analysis.list"
res <- GET(url = url,
query = list(sdate="2021-10-28",
edate="2021-10-28",
order_type="",
now_page=1))
# 테이블
res %>%
read_html(encoding = "EUC-KR") %>%
html_node(css = "div.table_style01") %>%
html_table() -> data01
# 첨부파일 링크
res %>%
read_html(encoding = "EUC-KR") %>%
html_nodes(css = "div.dv_input > a") %>%
html_attr("href") -> pdf_links
# pdf 타이틀
res %>%
read_html(encoding = "EUC-KR") %>%
html_nodes(css = "div.dv_input > a") %>%
html_attr("title") -> pdf_titles
# 데이터 추가
data01$첨부파일 = paste0("http://consensus.hankyung.com/", pdf_links)
data01$파일제목 = pdf_titles
# 다운로드
for (i in 1:nrow(data01)) {
download.file(url = data01$첨부파일[i],
destfile = paste0("pdf/", data01$파일제목[i]),
mode = "wb")
}
# list files
list.files("pdf/")
library(tabulizer)
library(pdftools)
library(magrittr)
library(stringr)
library(dplyr)
library(tidytext)
library(KoNLP)
library(wordcloud)
library(ggplot2)
file_lists = list.files("pdf/")
file_names = gsub(".pdf$", "", file_lists)
# text 추출
raw_txt = extract_text(file = paste0("pdf/", file_lists[1]), pages = 1)
article_df = data.frame(
file_names = file_names[1],
raw_text = raw_txt
)
# 데이터 프레임
articles = article_df %>%
mutate(clean_text = str_replace_all(raw_text, "[^가-힣]", " "),
clean_text = str_squish(clean_text))
# 단어 분리
noun <- extractNoun(articles$clean_text)
noun <- Filter(function(x) {nchar(x)>=2}, noun)
wordcount <- table(noun)
sort(wordcount, decreasing = T)
palete <- brewer.pal(9,"Set3")
wordcloud(names(wordcount),
freq=wordcount,
scale=c(5,1),
rot.per=0.25,
min.freq=1,
random.order=F,
random.color=T,
family = "AppleGothic",
colors=palete)
# 테이블 추출
df = extract_areas(file = paste0("pdf/", file_lists[1]), pages = 1)
df %>% as.data.frame() -> data
names(data) <- as.character(unlist(data[1, ]))
data = data[-1, ]
library(tabulizer)
library(pdftools)
library(magrittr)
library(stringr)
library(dplyr)
library(tidytext)
library(KoNLP)
library(wordcloud)
library(ggplot2)
install.packages("KoNLP")
install.packages("koNLP")
library(lubridate)
library(httr)
library(urltools)
library(rvest)
library(tidyverse)
library(jsonlite)
# 테스트를 위해 오늘 날짜를 지정합니다.
today <- Sys.Date() %>% as.character()
# 퍼센트 인코딩 함수가 담긴 R 파일을 불러옵니다.
source(file = './code/encodings.R')
# 검색어와 조회시작일자, 조회종료일자를 입력하면 조건에 맞는 블로그의 수를 반환하는
# 사용자 정의 함수를 만듭니다.
getBlogCnt <- function(searchWord, bgnDate, endDate) {
# HTTP 요청합니다.
res <- GET(url = 'https://section.blog.naver.com/ajax/SearchList.nhn',
query = list(countPerPage = '7',
currentPage = '1',
startDate = bgnDate,
endDate = endDate,
keyword = searchWord %>% pcntEncoding2Utf8() ),
add_headers(referer = 'https://section.blog.naver.com/BlogHome.nhn') )
# 응답 결과를 확인합니다.
# print(x = res)
# 응답 바디에 있는 ")]}',"을 제거하고 fromJSON() 함수에 할당합니다.
json <- res %>%
as.character() %>%
str_remove(pattern = "\\)\\]\\}\\',") %>%
fromJSON()
# 블로그 수를 추출한 다음 숫자 벡터로 변환합니다.
totalCount <- json$result$totalCount %>% as.numeric()
# 결과를 반환합니다.
return(totalCount)
}
# 오늘 날짜로 테스트합니다.
getBlogCnt(searchWord = '암호화폐',
bgnDate = today,
endDate = today)
# 검색어와 조회시작일자, 조회종료일자, 페이지수를 입력하면 조건에 맞는 블로그
# 데이터를 수집하여 데이터프레임으로 반환하는 사용자 정의 함수를 만듭니다.
getBlogDf <- function(searchWord, bgnDate, endDate, page = 1) {
# HTTP 요청합니다.
res <- GET(url = 'https://section.blog.naver.com/ajax/SearchList.nhn',
query = list(countPerPage = '7',
currentPage = page,
startDate = bgnDate,
endDate = endDate,
keyword = searchWord %>% pcntEncoding2Utf8() ),
add_headers(referer = 'https://section.blog.naver.com/BlogHome.nhn') )
# 응답 결과를 확인합니다.
# print(x = res)
# 응답 바디에 있는 ")]}',"을 제거하고 fromJSON() 함수에 할당합니다.
json <- res %>%
as.character() %>%
str_remove(pattern = "\\)\\]\\}\\',") %>%
fromJSON()
# 데이터프레임을 출력합니다.
df <- json$result$searchList
# 필요한 컬럼만 남깁니다.
df <- df[, c('blogId', 'postUrl', 'noTagTitle', 'contents', 'nickName', 'blogName', 'addDate')]
# 간단한 전처리를 실시합니다.
# contents 컬럼에 있는 태그와 불필요한 기호(&quot;)를 제거합니다.
df$contents <- df$contents %>% str_remove_all(pattern = '<.+?>|&quot;')
# addDate 컬럼에 날짜 대신 숫자 벡터로 들어 있으므로 1000으로 나눈 후,
# POSIXct로 속성을 바꿔줍니다.
df$addDate <- as.POSIXct(x = df$addDate / 1000, origin = '1970-01-01')
# 결과를 반환합니다.
return(df)
}
# 오늘 날짜로 테스트합니다.
df <- getBlogDf(searchWord = '암호화폐',
bgnDate = today,
endDate = today,
page = 1)
# 결과를 출력합니다.
print(x = df)
# 검색어와 조회시작일자, 조회종료일자를 지정하면 해당 조건의 모든 블로그 데이터를
# 수집하는 반복문이 포함된 사용자 정의 함수를 만듭니다.
getAllBlogDf <- function(searchWord, bgnDate, endDate) {
# 조건에 맞는 블로그 수를 가져옵니다.
blogCnt <- getBlogCnt(searchWord = searchWord, bgnDate = bgnDate, endDate = endDate)
# 페이지 수를 계산합니다.
pages <- ceiling(x = blogCnt / 7)
# 블로그 수와 페이지 수를 출력합니다.
cat('> 블로그 수는', blogCnt, '& 페이지 수는', pages, '입니다.\n')
# 만약 블로그의 수가 0이면 아래 라인을 실행하지 않습니다.
if (blogCnt >= 1) {
# 최종 결과 객체를 빈 데이터프레임으로 생성합니다.
result <- data.frame()
# 반복문을 실행합니다.
for (page in 1:pages) {
# 현재 진행상황을 출력합니다.
cat('>> 현재', page, '페이지 실행 중입니다.\n')
# 해당 페이지의 블로그 데이터를 수집한 다음 df에 할당합니다.
df <- getBlogDf(searchWord = searchWord,
bgnDate = bgnDate,
endDate = endDate,
page = page)
# 최종 결과 객체에 추가합니다.
result <- rbind(result, df)
# 1초간 멈춥니다.
Sys.sleep(time = 1)
}
# 최종 결과를 반환합니다.
return(result)
}
}
# 오늘 날짜로 테스트합니다.
blogData <- getAllBlogDf(searchWord = '암호화폐',
bgnDate = today,
endDate = today)
# 결과를 출력합니다.
print(x = blogData)
# 오늘 날짜로 테스트합니다.
blogData <- getAllBlogDf(searchWord = '암호화폐',
bgnDate = today,
endDate = today)
# 검색어를 설정합니다.
keyword <- '암호화폐'
# dates 객체를 출력합니다.
print(x = dates)
# 반복문을 실행합니다.
for (date in dates) {
# 반복문 안에서 date가 날짜 벡터에서 숫자 벡터로 자동 변환되는
# 문제가 발생하므로 다시 날짜 벡터로 강제 변환합니다.
date <- date %>% as.Date(origin = '1970-01-01')
# 현재 진행상황을 출력합니다.
date4print <- format(x = date, format = '%Y년 %m월 %d일에')
cat('현재', date4print, '등록된 블로그를 수집하고 있습니다.\n')
# 해당 일자에 등록된 모든 블로그를 수집합니다.
blogData <- getAllBlogDf(searchWord = keyword,
bgnDate = date,
endDate = date)
# 최종 결과 객체에 추가합니다.
result <- rbind(result, blogData)
# 개행을 추가합니다.
cat('\n')
# 1초간 멈춥니다.
Sys.sleep(time = 1)
}
# 결과를 출력합니다.
print(x = blogData)
# 조회시작일과 조회종료일로부터 벡터를 생성합니다.
dates <- seq(from = ymd('2021-01-01'), to = ymd('2021-10-03'), by = '1 day')
# 최종 결과 객체를 빈 데이터프레임으로 생성합니다.
result <- data.frame()
# 최종 결과 객체의 구조를 파악합니다.
str(object = result)
View(df)
library(dplyr)
library(tuber)
library(ggplot2)
library(RAthena)
install.packages("RAthena")
install.packages("DBI")
library(DBI)
library(RAthena)
install.packages("quantmod")
library(quantmod)
library(ggplot2)
library(dplyr)
# KOSPI 지수의 ticker Symbol ^KS11
KOSPI = getSymbols("^KS11",
from = "2001-01-01"
to = Sys.time()
)
# KOSPI 지수의 ticker Symbol ^KS11
KOSPI = getSymbols("^KS11",
from = "2001-01-01",
to = Sys.time(),
auto.assign = FALSE)
head(KOSPI)
tail(KOSPI)
samsung = getSymbols("005930.K",
from = "2001-01-01",
to = Sys.time(),
auto.assign = FALSE)
Apple = getSymbols("AAPL",
from = "2001-01-01",
to = Sys.time(),
auto.assign = FALSE)
str(KOSPI)
data.frame(date= time(KOSPI),
KOSPI)
data.frame(date= time(KOSPI),
KOSPI,
isgrowth = ifelse(Cl(KOSPI) > Op(KOSPI), "up", 'down')
)
data =data.frame(date= time(KOSPI),
KOSPI,
isgrowth = ifelse(Cl(KOSPI) > Op(KOSPI), "up", 'down')
)
glimpse(data)
head(data)
colnames(data) = c("date", "Open", "High", "Low", "Close", "Volume", "Adjusted", "isgrowth")
head(data)
)
theme_minimal()
View(Apple)
# Step 03 ---- 데이터 시각화 ----
# 시각화
head(data %>% filter(date("2020-01-01"))
# Step 03 ---- 데이터 시각화 ----
# 시각화
head(data %>% filter(date >= "2020-01-01"))
glimpse(data)
library(ggplot)
install.packages("ggplot")
library(ggplot2)
# 가법모형 시계열 분해
# yt = 계절성 요인(S) + 추세 순환 요인(T) + 불규칙 요인(R)
?decompose
install.packages("forecast")
# Step 06 ---- 시계열 회귀 모형 ----
# 추세 요인 반영
# y = b0 + b1(추세 요인t) + b2(계절성 요인t) + et
# t = 1, ..., T(시간)
# tslm() 함수 이용 (trend) & 계절성 요인(Season)
?tslm
# Step 06 ---- 시계열 회귀 모형 ----
# 추세 요인 반영
# y = b0 + b1(추세 요인t) + b2(계절성 요인t) + et
# t = 1, ..., T(시간)
# tslm() 함수 이용 (trend) & 계절성 요인(Season)
??tslm
summary(fit_lm)
# Step 01 ---- KOSPI 데이터 불러오기 ----
library(dplyr)
library(quantmod)
library(ggplo2)
library(forcast)
#> # KPSS Unit Root Test #
#> #######################
#>
#> Test is of type: mu with 7 lags.
#>
#> Value of test-statistic is: 10.72
#>
#> Critical value for a significance level of:
#>                 10pct  5pct 2.5pct  1pct
#> critical values 0.347 0.463  0.574 0.739
# KOSPI 지수의 ticker Symbol ^KS11
# 애플: AAPL
# 삼성전자: 005930.KS
KOSPI =
# Step 02 ---- 정상성과 차분 ----
# ARIMA 정상성(Stationary)과 차분(Differencing)
# 정상성
# 참고자료: https://otexts.com/fppkr/stationarity.html
# -- 시간의 흐름에 영향을 받지 않음
# -- 계절성 및 추세가 있는 시계열은 정상성을 나타내는 시계열은 아님.
# 차분: 정상성이 없는 시계열에 정상성을 나타낼 수 있도록 만드는 방법.
# -- 추세나 계절성이 제거(또는 감소)됨.
# -- 차분 계산식: y = y(t) - y(t-1)
# 정상성 확인하는 방법: 자기상관함수(Auto Correlation Function: ACF)
# -- 정상성이 있다면 ACF가 0으로 비교적 빠르게 떨어짐
# 단위근 검정(Unit Root Tests): 차분을 할지, 또는 얼마나 많이 해야 할지 판단.
# -- 단위근 검정 수행 위해서는 URCA 패키지 활용.
# -- KPSS 단위근 검정 사용
# -- 참고자료: https://otexts.com/fppkr/stationarity.html#%EB%8B%A8%EC%9C%84%EA%B7%BC%EA%B2%80%EC%A0%95
# Step 03 ---- 정상성과 차분 ----
library(urca)
kpss_test - ur.kpss(ts_kospi)
autoplot(a10)
h02 %>% ets() %>% forecast() %>% summary()
autoplot(Apple)
h02 %>% ets() %>% forecast() %>% summary()
autoplot(Apple)
autoplot(data)
h02 %>% ets() %>% forecast() %>% summary()
View(Apple)
View(Apple)
# Step 01 ---- KOSPI 데이터 불러오기 ----
library(quantmod)
# KOSPI 지수의 ticker Symbol ^KS11
# 애플: AAPL
# 삼성전자: 005930.KS
install.packages("CausalImppact")
# KOSPI 지수의 ticker Symbol ^KS11
# 애플: AAPL
# 삼성전자: 005930.KS
install.packages("CausalImpact")
